{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning. Gensim is implemented in Python and Cython.\n",
    "\n",
    "Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community.\n",
    "\n",
    "Features\n",
    "1. All algorithms are memory-independent w.r.t. the corpus size (can process input larger than RAM, streamed, out-of-core)\n",
    "2. Intuitive interfaces\n",
    "\n",
    "     a)easy to plug in your own input corpus/datastream (simple streaming API)\n",
    "     \n",
    "     b)easy to extend with other Vector Space algorithms (simple transformation API)\n",
    "     \n",
    "3. Efficient multicore implementations of popular algorithms, such as online Latent Semantic Analysis (LSA/LSI/SVD), Latent Dirichlet Allocation (LDA), Random Projections (RP), Hierarchical Dirichlet Process (HDP) or word2vec deep learning.\n",
    "4. Distributed computing: can run Latent Semantic Analysis and Latent Dirichlet Allocation on a cluster of computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A)Gensim stands for \"Generate Similar\"\n",
    "\n",
    "B)Features provided by Gensim :\n",
    "\n",
    "1. fastText\n",
    "2. word2vec\n",
    "3. LSA\n",
    "4. LDA\n",
    "5. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.0.0-cp38-cp38-win_amd64.whl (23.8 MB)\n",
      "Requirement already satisfied: Cython==0.29.21 in c:\\users\\piyush.pathak\\anaconda3\\lib\\site-packages (from gensim) (0.29.21)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.2.0.tar.gz (119 kB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\piyush.pathak\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\piyush.pathak\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-4.2.0-py3-none-any.whl size=109637 sha256=d74014803ab3206ff1e00dc056af2f58e470b74b4d01a30696b2f5bc92fac074\n",
      "  Stored in directory: c:\\users\\piyush.pathak\\appdata\\local\\pip\\cache\\wheels\\24\\f6\\ea\\70a0761bdfaeacff66662751fe71920e25c4c43d97098a3886\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.0.0 smart-open-4.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documents : It refers to some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Akshay is teaching gensim on youtube.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus : It refers to collection of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Akshay is teaching gensim on youtube.\",\"Today is a sunny day\",\"India is one of the top ranking teasm in cricket\",\"My favourite hobby is playing badminton\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "processed_corpus = [[word for word in document.lower().split() if word not in stoplist]\n",
    "   for document in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['akshay', 'is', 'teaching', 'gensim', 'on', 'youtube.'],\n",
      " ['today', 'is', 'sunny', 'day'],\n",
      " ['india', 'is', 'one', 'top', 'ranking', 'teasm', 'cricket'],\n",
      " ['my', 'favourite', 'hobby', 'is', 'playing', 'badminton']]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "corpus = \"\"\"'Akshay is teaching gensim on youtube.',\"Today is a sunny day\",\"India is one of the top ranking teasm in cricket\",'My favourite hobby is playing badminton'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['akshay',\n",
       " 'is',\n",
       " 'teaching',\n",
       " 'gensim',\n",
       " 'on',\n",
       " 'youtube',\n",
       " 'today',\n",
       " 'is',\n",
       " 'sunny',\n",
       " 'day',\n",
       " 'india',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'top',\n",
       " 'ranking',\n",
       " 'teasm',\n",
       " 'in',\n",
       " 'cricket',\n",
       " 'my',\n",
       " 'favourite',\n",
       " 'hobby',\n",
       " 'is',\n",
       " 'playing',\n",
       " 'badminton']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.utils.simple_preprocess(corpus, deacc=False, min_len=2, max_len=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Document is text and vector is a mathematically convenient representation of that text.\n",
    "\n",
    "One more important thing to be noted here is that, two different documents may have the same vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(20 unique tokens: ['akshay', 'gensim', 'is', 'on', 'teaching']...)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'akshay': 0,\n",
      " 'badminton': 15,\n",
      " 'cricket': 9,\n",
      " 'day': 6,\n",
      " 'favourite': 16,\n",
      " 'gensim': 1,\n",
      " 'hobby': 17,\n",
      " 'india': 10,\n",
      " 'is': 2,\n",
      " 'my': 18,\n",
      " 'on': 3,\n",
      " 'one': 11,\n",
      " 'playing': 19,\n",
      " 'ranking': 12,\n",
      " 'sunny': 7,\n",
      " 'teaching': 4,\n",
      " 'teasm': 13,\n",
      " 'today': 8,\n",
      " 'top': 14,\n",
      " 'youtube.': 5}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['akshay', 'is', 'teaching', 'gensim', 'on', 'youtube.'],\n",
       " ['today', 'is', 'sunny', 'day'],\n",
       " ['india', 'is', 'one', 'top', 'ranking', 'teasm', 'cricket'],\n",
       " ['my', 'favourite', 'hobby', 'is', 'playing', 'badminton']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
      " [(2, 1), (6, 1), (7, 1), (8, 1)],\n",
      " [(2, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)],\n",
      " [(2, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BoW_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(BoW_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=4, num_nnz=23)\n",
      "[(0, 0.7071067811865475), (9, 0.7071067811865475)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(BoW_corpus)\n",
    "words = \"akshay cricket\".lower().split()\n",
    "print(tfidf)\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "corpus = \"In terms of unforgettable looks, and enduring desire from enthusiasts who may have grown up gluing together the AMT 3-in-1 model kit that it inspired, the 1940 models stand today as some of the most iconic, instantly recognizable automobiles that the Ford Motor Company ever produced. That year, Fords were produced in two series: Standard and Deluxe. The easiest way to tell them apart is to look for a cleaner one-piece grille on Standard models, while the Deluxe version has a three-piece grille assembly. Both cars also had slightly different pieces of hood trim. This 1940 Ford Standard Tudor sedan was a very popular model that year–around 151,000 of them were built and sold. This Standard has been under the same California ownership since 1994, after the seller bought it from an owner in Texas. The seller describes the car as being entirely original, though the age of the finish and status of any restoration or refresh are unknown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentence = sent_tokenize(corpus)\n",
    "\n",
    "list_of_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_simple_preprocess_data = []\n",
    "for i in list_of_sentence:\n",
    "    list_of_simple_preprocess_data.append(gensim.utils.simple_preprocess(i, deacc=True, min_len=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list_of_simple_preprocess_data\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(list_of_simple_preprocess_data)\n",
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import lemmatize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_texts(texts):\n",
    "    texts = [[word for word in line if word not in stops] for line in texts]\n",
    "    texts = [bigram[line] for line in texts]\n",
    "    texts = [[word.decode(\"utf-8\").split('/')[0] for word in lemmatize(' '.join(line), allowed_tags=re.compile('(NN)'), min_length=5)] for line in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "train_texts = process_texts(list_of_simple_preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary = Dictionary(train_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in train_texts]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = LdaModel(corpus=corpus, num_topics=2, id2word=dictionary)\n",
    "\n",
    "ldamodel.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
